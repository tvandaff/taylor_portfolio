<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Project 3: Fairness | Taylor Sullivan</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.71.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://tvandaff.github.io/taylor_portfolio/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Project 3: Fairness" />
<meta property="og:description" content="For this project, I systematically identify fairness issues in AI-enabled systems and think through potential fairness problems in a credit scoring scenario and in a movie streaming scenario. Specifically, I (1) identified potential harms that can be caused by an unfair AI system, (2) identified potential sources of bias, (3) analyzed and improved fairness of a classifier, and (4) discussed possible fairness practices throughout the system&#39;s life cycle." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tvandaff.github.io/taylor_portfolio/post/project-3/" />
<meta property="article:published_time" content="2019-03-01T11:13:32-04:00" />
<meta property="article:modified_time" content="2019-03-01T11:13:32-04:00" />
<meta itemprop="name" content="Project 3: Fairness">
<meta itemprop="description" content="For this project, I systematically identify fairness issues in AI-enabled systems and think through potential fairness problems in a credit scoring scenario and in a movie streaming scenario. Specifically, I (1) identified potential harms that can be caused by an unfair AI system, (2) identified potential sources of bias, (3) analyzed and improved fairness of a classifier, and (4) discussed possible fairness practices throughout the system&#39;s life cycle.">
<meta itemprop="datePublished" content="2019-03-01T11:13:32-04:00" />
<meta itemprop="dateModified" content="2019-03-01T11:13:32-04:00" />
<meta itemprop="wordCount" content="677">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Project 3: Fairness"/>
<meta name="twitter:description" content="For this project, I systematically identify fairness issues in AI-enabled systems and think through potential fairness problems in a credit scoring scenario and in a movie streaming scenario. Specifically, I (1) identified potential harms that can be caused by an unfair AI system, (2) identified potential sources of bias, (3) analyzed and improved fairness of a classifier, and (4) discussed possible fairness practices throughout the system&#39;s life cycle."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://tvandaff.github.io/taylor_portfolio/images/fairness2.PNG');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://tvandaff.github.io/taylor_portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Taylor Sullivan
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      






<a href="https://www.youtube.com/channel/UCxIrcHNV_pTfLRynppSyG4g/" target="_blank" class="link-transition youtube link dib z-999 pt3 pt0-l mr1" title="Youtube link" rel="noopener" aria-label="follow on Youtube——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527,41.34c-0.278,0-0.478,0.078-0.6,0.244  c-0.121,0.156-0.18,0.424-0.18,0.796v0.896h1.543V42.38c0-0.372-0.062-0.64-0.185-0.796C42.989,41.418,42.792,41.34,42.527,41.34z   M36.509,41.309c0.234,0,0.417,0.076,0.544,0.23c0.123,0.155,0.185,0.383,0.185,0.682v4.584c0,0.286-0.053,0.487-0.153,0.611  c-0.1,0.127-0.256,0.189-0.47,0.189c-0.148,0-0.287-0.033-0.421-0.096c-0.135-0.062-0.274-0.171-0.415-0.313v-5.531  c0.119-0.122,0.239-0.213,0.36-0.271C36.26,41.335,36.383,41.309,36.509,41.309z M41.748,44.658v1.672  c0,0.468,0.057,0.792,0.17,0.974c0.118,0.181,0.313,0.269,0.592,0.269c0.289,0,0.491-0.076,0.606-0.229  c0.114-0.153,0.175-0.489,0.175-1.013v-0.405h1.795v0.456c0,0.911-0.217,1.596-0.657,2.059c-0.435,0.459-1.089,0.687-1.958,0.687  c-0.781,0-1.398-0.242-1.847-0.731c-0.448-0.486-0.676-1.157-0.676-2.014v-3.986c0-0.768,0.249-1.398,0.742-1.882  c0.493-0.484,1.128-0.727,1.911-0.727c0.799,0,1.413,0.225,1.843,0.674c0.429,0.448,0.642,1.093,0.642,1.935v2.264H41.748z   M38.623,48.495c-0.271,0.336-0.669,0.501-1.187,0.501c-0.343,0-0.646-0.062-0.912-0.192c-0.267-0.129-0.519-0.327-0.746-0.601  v0.681h-1.764V36.852h1.764v3.875c0.237-0.27,0.485-0.478,0.748-0.616c0.267-0.143,0.534-0.212,0.805-0.212  c0.554,0,0.975,0.189,1.265,0.565c0.294,0.379,0.438,0.933,0.438,1.66v4.926C39.034,47.678,38.897,48.159,38.623,48.495z   M30.958,48.884v-0.976c-0.325,0.361-0.658,0.636-1.009,0.822c-0.349,0.191-0.686,0.282-1.014,0.282  c-0.405,0-0.705-0.129-0.913-0.396c-0.201-0.266-0.305-0.658-0.305-1.189v-7.422h1.744v6.809c0,0.211,0.037,0.362,0.107,0.457  c0.077,0.095,0.196,0.141,0.358,0.141c0.128,0,0.292-0.062,0.488-0.188c0.197-0.125,0.375-0.283,0.542-0.475v-6.744h1.744v8.878  H30.958z M24.916,38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916z M32.994,32.978c0-0.001,12.08,0.018,13.514,1.45  c1.439,1.435,1.455,8.514,1.455,8.555c0,0-0.012,7.117-1.455,8.556C45.074,52.969,32.994,53,32.994,53s-12.079-0.031-13.516-1.462  c-1.438-1.435-1.441-8.502-1.441-8.556c0-0.041,0.004-7.12,1.441-8.555C20.916,32.996,32.994,32.977,32.994,32.978z M42.52,29.255  h-1.966v-1.08c-0.358,0.397-0.736,0.703-1.13,0.909c-0.392,0.208-0.771,0.312-1.14,0.312c-0.458,0-0.797-0.146-1.027-0.437  c-0.229-0.291-0.345-0.727-0.345-1.311v-8.172h1.962v7.497c0,0.231,0.045,0.399,0.127,0.502c0.08,0.104,0.216,0.156,0.399,0.156  c0.143,0,0.327-0.069,0.548-0.206c0.22-0.137,0.423-0.312,0.605-0.527v-7.422h1.966V29.255z M31.847,27.588  c0.139,0.147,0.339,0.219,0.6,0.219c0.266,0,0.476-0.075,0.634-0.223c0.157-0.152,0.235-0.358,0.235-0.618v-5.327  c0-0.214-0.08-0.387-0.241-0.519c-0.16-0.131-0.37-0.196-0.628-0.196c-0.241,0-0.435,0.065-0.586,0.196  c-0.148,0.132-0.225,0.305-0.225,0.519v5.327C31.636,27.233,31.708,27.439,31.847,27.588z M30.408,19.903  c0.528-0.449,1.241-0.674,2.132-0.674c0.812,0,1.48,0.237,2.001,0.711c0.517,0.473,0.777,1.083,0.777,1.828v5.051  c0,0.836-0.255,1.491-0.762,1.968c-0.513,0.476-1.212,0.714-2.106,0.714c-0.858,0-1.547-0.246-2.064-0.736  c-0.513-0.492-0.772-1.152-0.772-1.983v-5.068C29.613,20.954,29.877,20.351,30.408,19.903z M24.262,16h-2.229l2.634,8.003v5.252  h2.213v-5.5L29.454,16h-2.25l-1.366,5.298h-0.139L24.262,16z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30  S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.linkedin.com/in/-taylorsullivan/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/tvandaff" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Project 3: Fairness</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              For this project, I systematically identify fairness issues in AI-enabled systems and think through potential fairness problems in a credit scoring scenario and in a movie streaming scenario. Specifically, I (1) identified potential harms that can be caused by an unfair AI system, (2) identified potential sources of bias, (3) analyzed and improved fairness of a classifier, and (4) discussed possible fairness practices throughout the system&#39;s life cycle.
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://tvandaff.github.io/taylor_portfolio/post/project-3/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://tvandaff.github.io/taylor_portfolio/post/project-3/&amp;text=Project%203:%20Fairness" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://tvandaff.github.io/taylor_portfolio/post/project-3/&amp;title=Project%203:%20Fairness" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Project 3: Fairness</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-03-01T11:13:32-04:00">Apr 1, 2022</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">
      <h2>Train Model</h2>
      <p> For this project, I used a dataset with various features necessary (or unnecessary) for determining whether to grant loans. I created an XGBClassifier trained on the dummy variable features and tuned using GridSearchCV. Once I achieved an accuracy of .756 on the test set, I was satisfied with the performance and moved on to measuring fairness.</p>
      <h2>Measuring Fairness</h2>
      <p>I first evaluated Anti-Classification with the model. For this metric, I first determines whether the datapoints are the same except for age and gender and then evaluate whether cases exist with differences in predictions. </p>
      <p><b>Anti-classification Results:</b> To operationalize this metric, I decided to first try to calculate the accuracy on rows that were duplicated except for gender and age characteristics, however these rows did not exist. Instead, I calculated the Pearson's Correlation coefficient across all of the variables to determine whether gender and age had additional proxies that were causing multicolinearity in the model, or else, were causing age and gender to be considered in the model predictions multiple times. Calculating the Pearson's Correlation Coefficient over model predicted y values also inadvertently shows the significance of each variable in model predictions.</p>

        <p>Based on the generated heatmap and correlation coefficients, it is clear that Age is slightly considered by the model when it calculates predictions, since there is a small correlation between age and prediction results (-.11). Housing is also moderately correlated with age at a coefficient of .3, meaning this could be treated as a proxy variable for age in the model's features. On the other hand, sex is not considered heavily by the model, with a coefficient of .072. The highest correlation sex has is with age, meaning these variables can have a compound impact on model predictions. Anti-classification is definitely not present in the current model based on these factors. To alleviate the model's consideration of protected categories, we need to remove these variables from consideration as well as the correlated variables.</p>
        
        <p>To further test the model for classification, I also generated random input data that only differed in age or gender to see if I was able to find inconsistencies in model predictions. To do this, I simply took the male data and changed the male designator to female and ran the data through the model. The percent change in model predictions was: 23.29% - meaning anti-classification is not present according to gender. I did the same thing to generate fake data points for age. The change in model predictions was: 4.25%, meaning the model does take this factor into account, but it does not have as much of an impact as gender. Anti-classification therefore does not exist for this protected category for the current model either.</p>

        <p>I then tested for group fairness. This checks whether the rates of prediction for each gender and age are the same.</p>

        <p><b>Group Fairness Results:</b> To operationalize group fairness, I decided to use positive predictive value. This would be able to clearly show if the model was incorrectly learning that the protected attributes were good indicators for credit risk. The positive predictive rate for males and females was relatively close, so I would venture to say the model meets group fairness for this attribute. This is a much different story for age. There were no credit risk predictions for students, the most for adults, and the less for young and seniors. Since there is a large disparity in positive predictive values across these groups, group fairness is not met for the age criteria.</p>

        <p>I lastly evaluate the model on separation to evaluate whether features are statistically independent of the prediction, given the target value, y.</p>
      
        <p><b>Separation Results:</b> To operationalize separation, I chose to calculate the false positive and false negative rates for each of the protected attributes. The discrepancies between group false positive and false negative rates are not as apparent across genders. However, in the age groups, there is much less separation in the false negative rate, than there is across the false positive rate, which was .07, .37, .13, and NaN for each of the respective age categories. Thus, we can conclusively say there is separation across the age category in the model.</p>
       
        <h2>Improve Model Fairness</h2>

        <p><b>Anti-classification report:</b> In order to improve this metric, I decided to remove gender and age from the model training. This was the most obvious approach to remove consideration of these variables.

          As it turns out, removing gender and age does not affect the accuracy at all. The same exact model is trained, meaning that there likely exists proxy variables in the remainder of the dataset. Since this is the case, anti-classification still does not exist and the percentage of discrepancies remains the same.</p>

        <p><b>Group Fairness report:</b> For this metric, I decided to train two separate models to try to adjust the threshold in accordance with the two separate genders as a secondary consequence. This threshold adjustment would ideally change the accuracy of each model in order to improve each gender metric individually so that they can converge on the same value.

          The positive predictive value for both female and male has decreased significantly. Changing the threshold inadvertently by optimizing two separate models for male and female groups actually decreased the accuracy of the model. The difference between the positive predictive values is the same: .02. This adjustment, therefore, did not work.</p>

        <p><b>Separation report:</b> I tried to improve separation by removing variables that were correlated with sex and gender. I removed the most highly correlated variables and each respective protected attribute.

          The false positive and negative rates did not change at all for either female or male groups. This may be because there are non-linear relationships between the variables that the model is capturing.</p>

      <h2>Fairness Properties</h2>
      <p>Each fairness metric comes with its own set of pros and cons. The anti-classification metric in particular, by virtue of not including protected attributes, promotes equity above all else. This is a fairness mindset that many have, however this comes at the expense of equality, which recognizes the disadvantages that people face based on their age, gender, etc. and attempts to offer the same opportunities equally across groups in spite of these disadvantages. This is the major tradeoff that anti-classification presents: the inability to offer disadvantaged groups different thresholds and offer an equal playing field. Anti-classification metrics in the case of credit risk, can inadvertently continue to discriminate against women and minorities because it never recognizes that many other attributes are linked to these factors or that these groups need to have different thresholds to improve equal access to credit. The goal of this model is equity: ensuring that the model is not able to detect protected attributes and each person has the same opportunity without regard for their circumstances or disadvantages.</p>
      <p>Group fairness, on the other hand, can allow developers to falsely categorize random individuals with positive outcomes instead of incentivizing the engineering of better models. The positives of this model are, that individuals from disadvantaged groups who may not be considered by a general model, but still would not default on a loan, can be considered and categorized with equality in mind. The goal of this metric is to provide equality: ensuring that each group of people has a similar rate of opportunity, and thus, inadvertently paying homage to the circumstances and disadvantages they faced leading up to the moment of decision.</p>
      <p>Separation also has its own pros and cons. For one, separation undervalues the true positive results. When models need to detect very rare diseases, etc., this could needlessly discriminate against a model that benefits most from a high true positive rate. On the other side, this metric is best for cases where false positives and/or negatives can cause people harm, and thus the model must to be aggressively trained to optimize against these errors. The goal of separation is to ensure that false positives and negatives are minimized. In the case of credit risk, this means that the risk of misclassifying a person as a good risk when they are bad is low, as well as the risk of misclassifying them as a bad risk when they are good is low.</p>
      <p>I would select separation as the metric. False negatives can greatly harm individuals who are in need of a loan, but the model denies incorrectly. This impacts their standard of living and potential to become homeowners/business owners/car owners. On the other hand, false positives result in the lending company losing an exorbitant amount of money and lost opportunity from interest. These are two extremes that the model should be safeguarded against: causing harm to others and loss of revenue for the lending company.</p>
      <h2>Fairness in Movie Recommendations</h2>
      <ol>
        <li>
          <p><b>Harms of representation/allocation etc.:</b> I would consider race, gender, and age as protected attributes.</p>
          <p>I would consider children as a special sub-population in particular within the age category. Gender is a protected category because we want to avoid showing women only romantic comedies or Hallmark movies and only showing educational content and documentaries to male viewers. This perpetuates stereotypes against women and their preferences that could result in negative self-image across women and lost educational opportunities (harm of representation). Because there is less educational content available to these women, this results in a harm of representation, where women, constantly being exposed only to shallow content, begin to view themselves as less in society. The harm of allocation exists when women are denied the initial opportunity to see the type of content they prefer, in this case documentaries, which is more of a transactional harm.</p>
          <p>This carries over to minority groups as well. We do not want to only recommend movies which actors that share the viewer's race. This causes a lack of valuable diversity across viewers and lost opportunity for quality content that viewers would enjoy despite this difference. The harm of allocation in this case is that individuals who would have really enjoyed a movie made by people of a different race, are not recommended this movie and it never appears on their screen, causing them to lose out on this enjoyable experience. The harm of representation runs much deeper. Because movie recommendations are essentially segregated, racism and prejudice remain major undertones in society and children grow up with the concept of "othering" people of different races.</p>
          <p>I would also consider age as a protected attribute, but with children as an unprotected separate binary feature in the dataset. By keeping age protected, we can avoid recommending only brand new movies to younger generations and vice a versa, when generational diversity can be very valuable in terms of educational value and exposure to unique cinematic techniques. The harm of allocation in this case is similar to the aforementioned ones. Elderly people do not have access to fresh content because the movie recommendation system assumes they are only interested in decades-old content, and they miss out on being able to see the new movie they wanted to see. The harm of representation, however, is that this constant denial of fresh content to the elderly causes them to constantly remain "out of the loop," missing pop culture references and they grow to be outcasts in society, discarded as old relics.</p>
        </li>
        <li> Five sources of bias:
          <ol>
            <li><i>Skewed samples:</i> Skewed samples could occur if females primarily select romantic comedies in historical data that the company has collected. They rarely choose documentaries and the opposite is true for males. Most women choose romantic comedies but there are occasions in which they choose something else to watch. Skew can certainly be the case in movie recommendations because society encourages certain preferences based on gender and race, which is difficult to overcome.</li>
            <li><i>Tainted examples:</i> These examples occur from data poisoning, which happens when an attacker inserts false data into the model to distort predictions and reduce accuracy. This is a risk for movie recommendation companies. There are many competitors who would like to reduce the capabilities of Netflix, etc. by disabling their ability to generate good recommendations and thereby encouraging customers to use their streaming service instead. These attacks must be safeguarded against with the security protocols we learned about in lecture.</li>
            <li><i>Limited features:</i> This occurs when a new user enters the movie recommendation platform and all we know about them is their name, address, and payment method. Until they continue to use the platform, rate movies, engage with content, etc., we won't be able to generate quality recommendations. When a movie recommendation platform is first launched, this is true across the entire platform. It must be mitigated possibly through general-populace recommendations that are most-likely to work for every one until more data can be gathered.</li>
            <li><i>Sample size disparity:</i> This occurs when there is a very small amount of users engaging with the platform within a certain race, country, gender, or age-group. Because this is the case, it is difficult to generalize your recommendations from that small sample to the remainder of that sub-population. This can be mitigated through the generation of additional data points artificially, or by marketing the platform more heavily to these subpopulations to improve the platform's diversity and amount of data for this sub-population.</li>
            <li><i>Proxies:</i> This occurs when there are features in the data that are highly correlated with protected attributes. Height is highly correlated with gender, so including both in a model results in multi-collinearity, poor model training, and unexpected/biased predictions. For movie recommendations, this can be the user's address and payment type may be highly correlated features indicating their income level. Including these features should not be relevant to the movie recommendations, but certainly both should not be included. This can occur in movie recommendations when we do not carefully preprocess the data, conduct our pre-processing correlation statistics, and drop correlated variables prior to training the model.</li>
          </ol>
        </li>
        <li>
          <b>One engineering practice improvement:</b> I would use the re-weighing technique present in AIF360 to reduce the bias in the dataset. This is a preprocessing technique that takes a sample of data that is imbalanced across its prtected attributes, and re-weights the examples in each (group, label) pair differently to ensure fairness before classification. I think this metric is particularly helpful when there are many under-represented groups present. In the case of a movie recommendation system, which is ever-expanding to new countries. The people from the new countries could be unfairly discriminated against in the recommendations if their smaller sample-size isn't adjusted for in the pre-processing stages. I would also remove features that are highly correlated with any of the protected attributes, to reduce their effect on movie recommendations. This would likely inprove group fairness as a result. I think these pre-processsing techniques are key because a model is only as good and fair as its data, so if we take care to ensure that the data is properly cleaned and processed prior to model training and architecture development, we can improve our chances of devising a truly fair model. If you instead choose a post-processing technique, such as thresholding, you are still inevitably stuck with a biased model that you are just artificially obscuring.
        </li>
      </ol>
      <h2>Code</h2>
      <script src="https://gist.github.com/tvandaff/cd6b1beb0696ca696d5cc4929dc98df1.js"></script>
<p><a href="https://github.com/tvandaff/FairnessinAISystems">Link to GitHub Repository</a></p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://tvandaff.github.io/taylor_portfolio" >
    &copy;  Taylor Sullivan 2022
  </a>
    <div>






<a href="https://www.youtube.com/channel/UCxIrcHNV_pTfLRynppSyG4g/" target="_blank" class="link-transition youtube link dib z-999 pt3 pt0-l mr1" title="Youtube link" rel="noopener" aria-label="follow on Youtube——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527,41.34c-0.278,0-0.478,0.078-0.6,0.244  c-0.121,0.156-0.18,0.424-0.18,0.796v0.896h1.543V42.38c0-0.372-0.062-0.64-0.185-0.796C42.989,41.418,42.792,41.34,42.527,41.34z   M36.509,41.309c0.234,0,0.417,0.076,0.544,0.23c0.123,0.155,0.185,0.383,0.185,0.682v4.584c0,0.286-0.053,0.487-0.153,0.611  c-0.1,0.127-0.256,0.189-0.47,0.189c-0.148,0-0.287-0.033-0.421-0.096c-0.135-0.062-0.274-0.171-0.415-0.313v-5.531  c0.119-0.122,0.239-0.213,0.36-0.271C36.26,41.335,36.383,41.309,36.509,41.309z M41.748,44.658v1.672  c0,0.468,0.057,0.792,0.17,0.974c0.118,0.181,0.313,0.269,0.592,0.269c0.289,0,0.491-0.076,0.606-0.229  c0.114-0.153,0.175-0.489,0.175-1.013v-0.405h1.795v0.456c0,0.911-0.217,1.596-0.657,2.059c-0.435,0.459-1.089,0.687-1.958,0.687  c-0.781,0-1.398-0.242-1.847-0.731c-0.448-0.486-0.676-1.157-0.676-2.014v-3.986c0-0.768,0.249-1.398,0.742-1.882  c0.493-0.484,1.128-0.727,1.911-0.727c0.799,0,1.413,0.225,1.843,0.674c0.429,0.448,0.642,1.093,0.642,1.935v2.264H41.748z   M38.623,48.495c-0.271,0.336-0.669,0.501-1.187,0.501c-0.343,0-0.646-0.062-0.912-0.192c-0.267-0.129-0.519-0.327-0.746-0.601  v0.681h-1.764V36.852h1.764v3.875c0.237-0.27,0.485-0.478,0.748-0.616c0.267-0.143,0.534-0.212,0.805-0.212  c0.554,0,0.975,0.189,1.265,0.565c0.294,0.379,0.438,0.933,0.438,1.66v4.926C39.034,47.678,38.897,48.159,38.623,48.495z   M30.958,48.884v-0.976c-0.325,0.361-0.658,0.636-1.009,0.822c-0.349,0.191-0.686,0.282-1.014,0.282  c-0.405,0-0.705-0.129-0.913-0.396c-0.201-0.266-0.305-0.658-0.305-1.189v-7.422h1.744v6.809c0,0.211,0.037,0.362,0.107,0.457  c0.077,0.095,0.196,0.141,0.358,0.141c0.128,0,0.292-0.062,0.488-0.188c0.197-0.125,0.375-0.283,0.542-0.475v-6.744h1.744v8.878  H30.958z M24.916,38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916z M32.994,32.978c0-0.001,12.08,0.018,13.514,1.45  c1.439,1.435,1.455,8.514,1.455,8.555c0,0-0.012,7.117-1.455,8.556C45.074,52.969,32.994,53,32.994,53s-12.079-0.031-13.516-1.462  c-1.438-1.435-1.441-8.502-1.441-8.556c0-0.041,0.004-7.12,1.441-8.555C20.916,32.996,32.994,32.977,32.994,32.978z M42.52,29.255  h-1.966v-1.08c-0.358,0.397-0.736,0.703-1.13,0.909c-0.392,0.208-0.771,0.312-1.14,0.312c-0.458,0-0.797-0.146-1.027-0.437  c-0.229-0.291-0.345-0.727-0.345-1.311v-8.172h1.962v7.497c0,0.231,0.045,0.399,0.127,0.502c0.08,0.104,0.216,0.156,0.399,0.156  c0.143,0,0.327-0.069,0.548-0.206c0.22-0.137,0.423-0.312,0.605-0.527v-7.422h1.966V29.255z M31.847,27.588  c0.139,0.147,0.339,0.219,0.6,0.219c0.266,0,0.476-0.075,0.634-0.223c0.157-0.152,0.235-0.358,0.235-0.618v-5.327  c0-0.214-0.08-0.387-0.241-0.519c-0.16-0.131-0.37-0.196-0.628-0.196c-0.241,0-0.435,0.065-0.586,0.196  c-0.148,0.132-0.225,0.305-0.225,0.519v5.327C31.636,27.233,31.708,27.439,31.847,27.588z M30.408,19.903  c0.528-0.449,1.241-0.674,2.132-0.674c0.812,0,1.48,0.237,2.001,0.711c0.517,0.473,0.777,1.083,0.777,1.828v5.051  c0,0.836-0.255,1.491-0.762,1.968c-0.513,0.476-1.212,0.714-2.106,0.714c-0.858,0-1.547-0.246-2.064-0.736  c-0.513-0.492-0.772-1.152-0.772-1.983v-5.068C29.613,20.954,29.877,20.351,30.408,19.903z M24.262,16h-2.229l2.634,8.003v5.252  h2.213v-5.5L29.454,16h-2.25l-1.366,5.298h-0.139L24.262,16z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30  S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.linkedin.com/in/-taylorsullivan/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/tvandaff" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://tvandaff.github.io/taylor_portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
