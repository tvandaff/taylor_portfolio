<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Project 5: Predicting User Contexts in Product Reviews | Taylor Sullivan</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.71.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://tvandaff.github.io/taylor_portfolio/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Project 5: Predicting User Contexts in Product Reviews" />
<meta property="og:description" content="Online consumers are overwhelmed by an increasing number of product options and a seemingly endless amount of available information. As a result, consumers must sort through countless product features to discover the criteria pertinent to their user needs and the context in which they plan to use the product. This is a tedious task that can elongate the shopping process by several hours. Though there are multiple systems designed to filter and group reviews with precompiled, or user-defined, keywords, currently an application capable of filtering reviews by user needs or contexts does not exist. Therefore, we propose an end-to-end system, which utilizes state-of-the-art Natural Language Processing techniques to extract user contexts and criteria from reviews, performs clustering and presents popular user contexts and important criteria for each context to incoming users. Our system supports review filtering based on user-selected contexts and/or criteria. We have validated our system with interview-style user studies to determine its potential educational value and helpfulness." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tvandaff.github.io/taylor_portfolio/post/project-5/" />
<meta property="article:published_time" content="2019-01-01T11:15:58-04:00" />
<meta property="article:modified_time" content="2019-01-01T11:15:58-04:00" />
<meta itemprop="name" content="Project 5: Predicting User Contexts in Product Reviews">
<meta itemprop="description" content="Online consumers are overwhelmed by an increasing number of product options and a seemingly endless amount of available information. As a result, consumers must sort through countless product features to discover the criteria pertinent to their user needs and the context in which they plan to use the product. This is a tedious task that can elongate the shopping process by several hours. Though there are multiple systems designed to filter and group reviews with precompiled, or user-defined, keywords, currently an application capable of filtering reviews by user needs or contexts does not exist. Therefore, we propose an end-to-end system, which utilizes state-of-the-art Natural Language Processing techniques to extract user contexts and criteria from reviews, performs clustering and presents popular user contexts and important criteria for each context to incoming users. Our system supports review filtering based on user-selected contexts and/or criteria. We have validated our system with interview-style user studies to determine its potential educational value and helpfulness.">
<meta itemprop="datePublished" content="2019-01-01T11:15:58-04:00" />
<meta itemprop="dateModified" content="2019-01-01T11:15:58-04:00" />
<meta itemprop="wordCount" content="383">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Project 5: Predicting User Contexts in Product Reviews"/>
<meta name="twitter:description" content="Online consumers are overwhelmed by an increasing number of product options and a seemingly endless amount of available information. As a result, consumers must sort through countless product features to discover the criteria pertinent to their user needs and the context in which they plan to use the product. This is a tedious task that can elongate the shopping process by several hours. Though there are multiple systems designed to filter and group reviews with precompiled, or user-defined, keywords, currently an application capable of filtering reviews by user needs or contexts does not exist. Therefore, we propose an end-to-end system, which utilizes state-of-the-art Natural Language Processing techniques to extract user contexts and criteria from reviews, performs clustering and presents popular user contexts and important criteria for each context to incoming users. Our system supports review filtering based on user-selected contexts and/or criteria. We have validated our system with interview-style user studies to determine its potential educational value and helpfulness."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://tvandaff.github.io/taylor_portfolio/images/911.png');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://tvandaff.github.io/taylor_portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Taylor Sullivan
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://tvandaff.github.io/taylor_portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      






<a href="https://www.youtube.com/channel/UCxIrcHNV_pTfLRynppSyG4g/" target="_blank" class="link-transition youtube link dib z-999 pt3 pt0-l mr1" title="Youtube link" rel="noopener" aria-label="follow on Youtube——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527,41.34c-0.278,0-0.478,0.078-0.6,0.244  c-0.121,0.156-0.18,0.424-0.18,0.796v0.896h1.543V42.38c0-0.372-0.062-0.64-0.185-0.796C42.989,41.418,42.792,41.34,42.527,41.34z   M36.509,41.309c0.234,0,0.417,0.076,0.544,0.23c0.123,0.155,0.185,0.383,0.185,0.682v4.584c0,0.286-0.053,0.487-0.153,0.611  c-0.1,0.127-0.256,0.189-0.47,0.189c-0.148,0-0.287-0.033-0.421-0.096c-0.135-0.062-0.274-0.171-0.415-0.313v-5.531  c0.119-0.122,0.239-0.213,0.36-0.271C36.26,41.335,36.383,41.309,36.509,41.309z M41.748,44.658v1.672  c0,0.468,0.057,0.792,0.17,0.974c0.118,0.181,0.313,0.269,0.592,0.269c0.289,0,0.491-0.076,0.606-0.229  c0.114-0.153,0.175-0.489,0.175-1.013v-0.405h1.795v0.456c0,0.911-0.217,1.596-0.657,2.059c-0.435,0.459-1.089,0.687-1.958,0.687  c-0.781,0-1.398-0.242-1.847-0.731c-0.448-0.486-0.676-1.157-0.676-2.014v-3.986c0-0.768,0.249-1.398,0.742-1.882  c0.493-0.484,1.128-0.727,1.911-0.727c0.799,0,1.413,0.225,1.843,0.674c0.429,0.448,0.642,1.093,0.642,1.935v2.264H41.748z   M38.623,48.495c-0.271,0.336-0.669,0.501-1.187,0.501c-0.343,0-0.646-0.062-0.912-0.192c-0.267-0.129-0.519-0.327-0.746-0.601  v0.681h-1.764V36.852h1.764v3.875c0.237-0.27,0.485-0.478,0.748-0.616c0.267-0.143,0.534-0.212,0.805-0.212  c0.554,0,0.975,0.189,1.265,0.565c0.294,0.379,0.438,0.933,0.438,1.66v4.926C39.034,47.678,38.897,48.159,38.623,48.495z   M30.958,48.884v-0.976c-0.325,0.361-0.658,0.636-1.009,0.822c-0.349,0.191-0.686,0.282-1.014,0.282  c-0.405,0-0.705-0.129-0.913-0.396c-0.201-0.266-0.305-0.658-0.305-1.189v-7.422h1.744v6.809c0,0.211,0.037,0.362,0.107,0.457  c0.077,0.095,0.196,0.141,0.358,0.141c0.128,0,0.292-0.062,0.488-0.188c0.197-0.125,0.375-0.283,0.542-0.475v-6.744h1.744v8.878  H30.958z M24.916,38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916z M32.994,32.978c0-0.001,12.08,0.018,13.514,1.45  c1.439,1.435,1.455,8.514,1.455,8.555c0,0-0.012,7.117-1.455,8.556C45.074,52.969,32.994,53,32.994,53s-12.079-0.031-13.516-1.462  c-1.438-1.435-1.441-8.502-1.441-8.556c0-0.041,0.004-7.12,1.441-8.555C20.916,32.996,32.994,32.977,32.994,32.978z M42.52,29.255  h-1.966v-1.08c-0.358,0.397-0.736,0.703-1.13,0.909c-0.392,0.208-0.771,0.312-1.14,0.312c-0.458,0-0.797-0.146-1.027-0.437  c-0.229-0.291-0.345-0.727-0.345-1.311v-8.172h1.962v7.497c0,0.231,0.045,0.399,0.127,0.502c0.08,0.104,0.216,0.156,0.399,0.156  c0.143,0,0.327-0.069,0.548-0.206c0.22-0.137,0.423-0.312,0.605-0.527v-7.422h1.966V29.255z M31.847,27.588  c0.139,0.147,0.339,0.219,0.6,0.219c0.266,0,0.476-0.075,0.634-0.223c0.157-0.152,0.235-0.358,0.235-0.618v-5.327  c0-0.214-0.08-0.387-0.241-0.519c-0.16-0.131-0.37-0.196-0.628-0.196c-0.241,0-0.435,0.065-0.586,0.196  c-0.148,0.132-0.225,0.305-0.225,0.519v5.327C31.636,27.233,31.708,27.439,31.847,27.588z M30.408,19.903  c0.528-0.449,1.241-0.674,2.132-0.674c0.812,0,1.48,0.237,2.001,0.711c0.517,0.473,0.777,1.083,0.777,1.828v5.051  c0,0.836-0.255,1.491-0.762,1.968c-0.513,0.476-1.212,0.714-2.106,0.714c-0.858,0-1.547-0.246-2.064-0.736  c-0.513-0.492-0.772-1.152-0.772-1.983v-5.068C29.613,20.954,29.877,20.351,30.408,19.903z M24.262,16h-2.229l2.634,8.003v5.252  h2.213v-5.5L29.454,16h-2.25l-1.366,5.298h-0.139L24.262,16z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30  S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.linkedin.com/in/-taylorsullivan/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/tvandaff" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Project 5: Predicting User Contexts in Product Reviews</h1>

          <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
            Team Members: Taylor Sullivan, Yifan Song, Weiyi Zhang
          </h2>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Online consumers are overwhelmed by an increasing number of product options and a seemingly endless amount of available information. As a result, consumers must sort through countless product features to discover the criteria pertinent to their user needs and the context in which they plan to use the product. This is a tedious task that can elongate the shopping process by several hours. Though there are multiple systems designed to filter and group reviews with precompiled, or user-defined, keywords, currently an application capable of filtering reviews by user needs or contexts does not exist. Therefore, we propose an end-to-end system, which utilizes state-of-the-art Natural Language Processing techniques to extract user contexts and criteria from reviews, performs clustering and presents popular user contexts and important criteria for each context to incoming users. Our system supports review filtering based on user-selected contexts and/or criteria. We have validated our system with interview-style user studies to determine its potential educational value and helpfulness.
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://tvandaff.github.io/taylor_portfolio/post/project-5/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://tvandaff.github.io/taylor_portfolio/post/project-5/&amp;text=Project%205:%20Predicting%20User%20Contexts%20in%20Product%20Reviews" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://tvandaff.github.io/taylor_portfolio/post/project-5/&amp;title=Project%205:%%20Predicting%20User%20Contexts%20in%20Product%20Reviews" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Project 5: Predicting User Contexts in Product Reviews</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-01-01T11:15:58-04:00">January 1, 2021</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">
    <h2>Abstract</h2>
    <p>Online consumers are overwhelmed by an increasing number of product options and a seemingly endless amount of available information. As a result, consumers must sort through countless product features to discover the criteria pertinent to their user needs and the context in which they plan to use the product. This is a tedious task that can elongate the shopping process by several hours. Though there are multiple systems designed to filter and group reviews with precompiled, or user-defined, keywords, currently an application capable of filtering reviews by user needs or contexts does not exist. Therefore, we propose an end-to-end system, which utilizes state-of-the-art Natural Language Processing techniques to extract user contexts and criteria from reviews, performs clustering and presents popular user contexts and important criteria for each context to incoming users. Our system supports review filtering based on user-selected contexts and/or criteria. We have validated our system with interview-style user studies to determine its potential educational value and helpfulness.</p>
    <h2>Introduction</h2>
    <p>When purchasing a product online, a common approach is to select the product with the highest star-rating. But for users with specific needs, a general star-rating cannot provide enough information. In other words, a product that works well for most users might not have a comparable performance under a specific context; such as, a projector that is good for movies might not work well as a golf simulator. Users in this situation are left to sort through hundreds of reviews to find a match for their specific needs or purposes.This elongates the product search, and the user experience suffers. This ineffective search process occurs because a good technique for searching through product reviews with respect to user contexts does not yet exist. We intend to provide a solution using context extraction and a context-driven, product-review search to improve the shopping experience for users with specific contexts.</p>
    <p>Besides the overall star rating, a user may also consider the often-provided product-feature (later referred to as criteria for the purposes of this paper) star-ratings for a product. For example, if a user intends to buy a projector to use as a golf simulator, the user will likely consider the star-ratings for brightness and lag-time, whereas another user intending to use the projector for movies will likely consider star-ratings for pixel-quality and screen-size. However, these criteria star-ratings are only helpful when the user is already familiar with the product and understands the relationship between the product's criteria and the user’s personal context. For users that are new to a product domain, this is not a realistic expectation, and consequently these users rely on inadequate review search-architecture and non-specific ratings to make a choice. This results in a continued lack of knowledge about the domain as well as an unsatisfactory product selection. Thus, we propose a review search-tool that combines context and criteria extraction to help users learn about an unfamiliar product/domain and improve the overall shopping experience. </p>
    <p>Despite extensive research, a similar effort does not yet exist and context extraction remains a novel task. Context extraction is most similar to the well-defined NLP task, aspect extraction (AE). AE focuses on extracting explicit aspects/criteria that are directly mentioned in a product review, whereas our context extraction model focuses on extracting user needs as well as the circumstances in which the product was used. Additionally, previous AE models have been finely tuned to extract from specific product categories and do not generalize well across unseen domains, whereas generalizability is a key feature of our model. Given this, we propose a new tool capable of extracting context and criteria from Amazon reviews in real time to provide users with an enhanced shopping experience, tailored to their individual needs. </p>
    <h2>Related Work</h2>
    <p>Our system aims to improve the online shopping experience and accelerate purchase decisions with the help of state-of-the-art Natural Language Processing techniques. In the first subsection, we will list several major challenges for online consumers and review existing interactive systems that support online purchase decision-making. In the second subsection, we introduce the Aspect Extraction (AE) task, which is the backbone of our system, and discuss previous works related to this topic.</p>
    <h3>Decision Supporting Interactive Systems</h3>
    <p>Research has shown that users are often overwhelmed by the number of shopping options, number of criteria, and number of resources available for amplifying product information. One possible solution is to automate an aggregation metric such as average ratings, present in most online shopping websites to this day. However, this is an oversimplification of the complexity present in product reviews \cite{article10}, the increasing number of fake reviews \cite{post18} and the different backgrounds and contexts each review contains\cite{searchlen}. According to the interview from \cite{mesh}, most users value raw reviews more than the generated output from an automated system. Therefore, it is more important to extract related reviews from the overwhelming resources rather than conducting simple aggregation and summarization.</p>
    <p>The comparison of product criteria to a user's own needs is also important to the research process. Another thread of research is focusing on building interactive interfaces to support decision making under multi-criteria and multi-option scenarios, such as faceted interfaces proposed in \cite{facet} and table-based visualization systems introduced in \cite{spreadsheet}, \cite{tablelen}, \cite{unakite} and \cite{focus}. One most familiar example may be the comparison table that Amazon uses to present a list of similar products. However, the majority of these approaches rely on a fixed set of criteria and neglect varying personal contexts and different user goals. To incorporate user participation, \cite{chen17} built a recommendation interface that allows user-defined criteria weights and evaluates the trade-off among different criteria, while remaining constrained to the fixed criteria set. To allow users to freely explore any product they desire, \cite{mesh} built Mesh which supported arbitrary search items and enabled consumers to build a comparison table progressively, capable of reflecting their own needs.</p>
    <p>However, though \cite{chen17} allowed users to develop their own weights for evaluation and \cite{mesh} supported highly flexible search tailored to a user's needs, these previous systems were contingent upon the assumption that users know what criteria matter most in their target contexts and that all users have a personal context in  mind. While these assumptions may serve well for users who are experts in the target product domain, these assumptions do not hold for novice users.</p>
    <h3>Extraction Models</h3>
    <p>The Aspect Extraction task has been explored both in unsupervised and supervised settings. The main unsupervised approaches involve frequent item mining and syntactic rule-based extraction. For example, \cite{zhuang06} and \cite{qiu11} built models that rely on pre-defined rules and thus work under the assumption that aspects are restricted to a small group of nouns. Other unsupervised approaches utilize topic modeling and Latent Dirichlet Allocation (LDA) based models \cite{titov08} \cite{zhao10}. In these models, the corpus is a mixture of aspects, and aspects are distributions over words. However, even though the corpus might be well described by the mixture of aspects, it is easy to arrive at poor-quality individual aspects.</p>
    <p>The traditional supervised approach typically uses Hidden Markov Models (HMM) and Conditional Random Fields (CRF) \cite{jakob10}. On top of this design basis, \cite{cheryn14}  introduced part-of-speech (POS) and named entity features, and \cite{toh16} incorporated syntactic features and word embeddings. These approaches won the aspect extraction task in 2014 and 2016 SemEval Challenge respectively.</p>
    <p>With the development of deep learning, neural networks have become one of the most preeminent techniques. \cite{liu15} and \cite{poria16} introduced Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) as solutions to AE. \cite{he17} incorporated an attention mechanism to improve the coherence of aspects by exploiting the distribution of word co-occurrences through neural word embeddings. \cite{wang17} proposed a multi-layer attention mechanism based model to jointly extract aspect and opinion terms. \cite{li18} further strengthened the joint model using truncated history-attention and a selective transformation network. In \cite{li19}, the authors designed a network to transfer the aspect knowledge learned from a coarse-grained network. Recently, BERT has emerged as one of the key innovations in NLP research \cite{devlin19}. Unsurprisingly, BERT-based models demonstrated competitive performance in Aspect Extraction.</p>
    <p>We have chosen two state of the art (SOTA) models to achieve a competitive performance in our context and criteria extraction tasks, one non-BERT based model and one BERT based. Prior to BERT, \cite{xu18} introduced Dual Embeddings CNN (DE-CNN), which integrates GloVe with domain-specific embeddings, resulting in a SOTA model of Aspect Extraction. This work showed that employing both (pre-trained) general purpose embeddings and embeddings trained from the domain-specific data could perform competitively and thus avoid the overly sophisticated approach prevalent in previous works. The second model is called BERT Post-Training (BERT-PT), which is currently one of the SOTA models for Aspect Extraction, and yet another simple but brilliant approach from \cite{xu19}. In BERT-PT, the authors propose a joint post-training technique that uses BERT’s pre-trained weights as the initialization for basic language understanding and post-train BERT with both domain knowledge and task knowledge data before fine-tuning.</p>
    <h2>Data</h2>
    <p>As it stands, there is no existing dataset suitable for training a context/criteria extraction model. Therefore, we needed to curate a new dataset. We developed this dataset with the Amazon 2018 review dataset \cite{amazon-19}, since Amazon is one of the most frequently visited e-commerce websites in the United States and its review dataset contains over 200 million reviews.</p>
    <p>We first picked 10 product categories from \emph{New York Times Wirecutter's} most popular product categories, attempting to capture a diverse set of meta categories - \emph{laptops (electronics), earbuds (electronics), speakers (electronics), routers (electronics), coffee machines (Home and Kitchen), vacuums (Home and Kitchen), mattresses (Home and Kitchen), bidet seats (Tools), pressure washers (Patio, Lawn and Garden), printers (Office Products)}. We then selected the top 30 reviews from each category ranked by helpfulness to form the foundation of our dataset, which we would subsequently annotate sentence-wise. We chose to have more categories and fewer reviews for the purpose of training a generalized, cross-domain extraction model, as opposed to a domain-specific model.</p>
    <p>We first annotated criteria, which we defined as the terms representing particular product aspects/features. The most similar datasets to this annotation task are the benchmark dataset for Aspect Extraction tasks, SemEval \cite{semeval-14} \cite{semeval-16}, which consist of review sentences with aspect terms labeled. For instance, in the sentence "\emph{The laptop has an incredible speed.}", the aspect is "\emph{speed}" and a positive sentiment is mentioned towards it. In AE task, the objective is to extract all aspects of the target entity, such as "\emph{I liked the \textbf{food}, but the \textbf{service} was bad.}" The aspect can also be a multi-word term (but should be treated as a single aspect) such as "\emph{The \textbf{hard disk} is too noisy.}." Here, "aspect" has a similar definition with what we defined as "criteria", however we additionally include "implicit criteria" whereas SemEval datasets do not. As an example, "\emph{This is printer is \textbf{heavy}}" where "heavy" is an adjective representing the "explicit" aspect, "weight". The SemEval dataset excluded "implicit" terms for the sake of simplicity, but we argue that these terms are valuable to a consumer's decision-making process.</p>
    <p>We then annotated the context within each review. We defined context as the terms representing user backgrounds, user needs/goals, and scenarios describing how a target entity (product category) was used. For instance, an annotated sentence from the review of vacuum reads, "\emph{The whole reason of ordering this was that I have a \textbf{back problem}, and a \textbf{dog}, so I needed a lightweight but efficient vacuum that would pick up all of the stuff from the \textbf{hardwood floors} and \textbf{carpet}.}" Due to the increased ambiguity of the context annotation task, we developed detailed annotation guidelines to insure precise and consistent annotations. These guidelines include instructions such as limiting the term length and part-of-speech tags. We also invited college students who no prior knowledge of the project to test the effectiveness of our guidelines. By reading the guidelines alone, they achieved ~80\% accuracy in comparison to our gold-standard dataset. </p>
    <p>Table 1 shows the statistics of our newly collected datasets. We noticed that context is sparser than criteria. We hypothesize that there are a fixed and fewer number of "contexts," but an ever-increasing number of product features as businesses continue to invent. We also hypothesize that customers are more willing to express their opinions on different aspects of products rather than divulging their personal needs or scenarios.</p>
    <figure>
      <img src="https://tvandaff.github.io/taylor_portfolio/images/capstonetable1.PNG"/> 
    </figure>
    <h2>Crowdsourcing</h2>
    <p>Our initial approach to data collection was an attempt at crowdsourcing through Amazon Mechanical Turk. We developed criteria and context annotation interfaces on the Meteor platform, which offered a scalable framework for our sentence annotation application. We iterated over our interface extensively to improve the quality of our interface, the clarity of the task, and decrease the latency incurred from querying our MongoDB database. </p>
    <p>Figure 1 depicts the crowdsourcing pipeline we used to collect our criteria and context annotation datasets. We filtered out AMT workers with less than a 95\% approval rating and had completed at least 1000 tasks. We required three annotations by three separate workers per review. Once a worker began the HIT, they were directed to our Meteor application, which consisted of a landing page that collected the worker’s ID number, a tutorial page which instructed the worker on how to complete the task, an annotation page where the user actually annotated three reviews in order to complete the task, and a finish page that gave the user a completion code to input into AMT.</p>
    <p>The interface queried the MongoDB database based on a sampling strategy which ordered the reviews by number of annotations, prevents a user from annotating the same review more than once, and stops cycling a review once three annotations have been collected. The results were collected with the MongoDB dataset and post processed using “majority vote” to select the criteria or context that was annotated by more than one worker. The workers were paid for their efforts, and our final dataset was completed. In our context collection phase, we added an additional worker necessary for annotation and allowed reviews to be annotated as “no context” by four workers before being filtered out. This greatly improved the precision of our dataset in the case of context because reviews were much more likely not to have a context and much more subjective than criteria. </p>
    <p>However, this collection process provided us with multiple challenges. We were only able to achieve high precision scores of 93\% and 76\% using a human evaluator, checking whether the Turker evaluated a similar portion of the text as the gold standard. This was a generous approach and not sustainable for a large dataset. It additionally resulted in poor performance on our initial models. Due to the poor-training of the model, we necessitated further iterations on the training data, which was costly both timewise and monetarily, causing us to abandon this approach. A high percentage of Turkers contacted the study to complain about the complexity of the task as well. Further work should be done to pursue this strategy as a method for collecting annotations by further simplifying the task and improving the instructions. </p>
    <h2></h2>
    <p></p>

<figure>
    <img src="https://tvandaff.github.io/taylor_portfolio/images/date911callanalysis.png"/> 
</figure>
<p><a href="https://github.com/tvandaff/911_call_analysis">Link to GitHub Repository</a></p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://tvandaff.github.io/taylor_portfolio" >
    &copy;  Taylor Sullivan 2022
  </a>
    <div>






<a href="https://www.youtube.com/channel/UCxIrcHNV_pTfLRynppSyG4g/" target="_blank" class="link-transition youtube link dib z-999 pt3 pt0-l mr1" title="Youtube link" rel="noopener" aria-label="follow on Youtube——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.527,41.34c-0.278,0-0.478,0.078-0.6,0.244  c-0.121,0.156-0.18,0.424-0.18,0.796v0.896h1.543V42.38c0-0.372-0.062-0.64-0.185-0.796C42.989,41.418,42.792,41.34,42.527,41.34z   M36.509,41.309c0.234,0,0.417,0.076,0.544,0.23c0.123,0.155,0.185,0.383,0.185,0.682v4.584c0,0.286-0.053,0.487-0.153,0.611  c-0.1,0.127-0.256,0.189-0.47,0.189c-0.148,0-0.287-0.033-0.421-0.096c-0.135-0.062-0.274-0.171-0.415-0.313v-5.531  c0.119-0.122,0.239-0.213,0.36-0.271C36.26,41.335,36.383,41.309,36.509,41.309z M41.748,44.658v1.672  c0,0.468,0.057,0.792,0.17,0.974c0.118,0.181,0.313,0.269,0.592,0.269c0.289,0,0.491-0.076,0.606-0.229  c0.114-0.153,0.175-0.489,0.175-1.013v-0.405h1.795v0.456c0,0.911-0.217,1.596-0.657,2.059c-0.435,0.459-1.089,0.687-1.958,0.687  c-0.781,0-1.398-0.242-1.847-0.731c-0.448-0.486-0.676-1.157-0.676-2.014v-3.986c0-0.768,0.249-1.398,0.742-1.882  c0.493-0.484,1.128-0.727,1.911-0.727c0.799,0,1.413,0.225,1.843,0.674c0.429,0.448,0.642,1.093,0.642,1.935v2.264H41.748z   M38.623,48.495c-0.271,0.336-0.669,0.501-1.187,0.501c-0.343,0-0.646-0.062-0.912-0.192c-0.267-0.129-0.519-0.327-0.746-0.601  v0.681h-1.764V36.852h1.764v3.875c0.237-0.27,0.485-0.478,0.748-0.616c0.267-0.143,0.534-0.212,0.805-0.212  c0.554,0,0.975,0.189,1.265,0.565c0.294,0.379,0.438,0.933,0.438,1.66v4.926C39.034,47.678,38.897,48.159,38.623,48.495z   M30.958,48.884v-0.976c-0.325,0.361-0.658,0.636-1.009,0.822c-0.349,0.191-0.686,0.282-1.014,0.282  c-0.405,0-0.705-0.129-0.913-0.396c-0.201-0.266-0.305-0.658-0.305-1.189v-7.422h1.744v6.809c0,0.211,0.037,0.362,0.107,0.457  c0.077,0.095,0.196,0.141,0.358,0.141c0.128,0,0.292-0.062,0.488-0.188c0.197-0.125,0.375-0.283,0.542-0.475v-6.744h1.744v8.878  H30.958z M24.916,38.6v10.284h-1.968V38.6h-2.034v-1.748h6.036V38.6H24.916z M32.994,32.978c0-0.001,12.08,0.018,13.514,1.45  c1.439,1.435,1.455,8.514,1.455,8.555c0,0-0.012,7.117-1.455,8.556C45.074,52.969,32.994,53,32.994,53s-12.079-0.031-13.516-1.462  c-1.438-1.435-1.441-8.502-1.441-8.556c0-0.041,0.004-7.12,1.441-8.555C20.916,32.996,32.994,32.977,32.994,32.978z M42.52,29.255  h-1.966v-1.08c-0.358,0.397-0.736,0.703-1.13,0.909c-0.392,0.208-0.771,0.312-1.14,0.312c-0.458,0-0.797-0.146-1.027-0.437  c-0.229-0.291-0.345-0.727-0.345-1.311v-8.172h1.962v7.497c0,0.231,0.045,0.399,0.127,0.502c0.08,0.104,0.216,0.156,0.399,0.156  c0.143,0,0.327-0.069,0.548-0.206c0.22-0.137,0.423-0.312,0.605-0.527v-7.422h1.966V29.255z M31.847,27.588  c0.139,0.147,0.339,0.219,0.6,0.219c0.266,0,0.476-0.075,0.634-0.223c0.157-0.152,0.235-0.358,0.235-0.618v-5.327  c0-0.214-0.08-0.387-0.241-0.519c-0.16-0.131-0.37-0.196-0.628-0.196c-0.241,0-0.435,0.065-0.586,0.196  c-0.148,0.132-0.225,0.305-0.225,0.519v5.327C31.636,27.233,31.708,27.439,31.847,27.588z M30.408,19.903  c0.528-0.449,1.241-0.674,2.132-0.674c0.812,0,1.48,0.237,2.001,0.711c0.517,0.473,0.777,1.083,0.777,1.828v5.051  c0,0.836-0.255,1.491-0.762,1.968c-0.513,0.476-1.212,0.714-2.106,0.714c-0.858,0-1.547-0.246-2.064-0.736  c-0.513-0.492-0.772-1.152-0.772-1.983v-5.068C29.613,20.954,29.877,20.351,30.408,19.903z M24.262,16h-2.229l2.634,8.003v5.252  h2.213v-5.5L29.454,16h-2.25l-1.366,5.298h-0.139L24.262,16z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30  S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://www.linkedin.com/in/-taylorsullivan/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/tvandaff" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://tvandaff.github.io/taylor_portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
